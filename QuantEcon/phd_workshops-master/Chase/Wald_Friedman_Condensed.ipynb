{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Problem that stumped Milton Friedman\n",
    "\n",
    "(and that Abraham Wald solved by inventing sequential analysis)\n",
    "\n",
    "**Authors**: Chase Coleman and Thomas J. Sargent, translated from Python by Alberto Polo\n",
    "\n",
    "**Abstract**:\n",
    "\n",
    "This notebook is a condensed version of the QuantEcon lecture [A Problem that Stumped Milton Friedman](https://lectures.quantecon.org/py/wald_friedman.html) which began with the corresponding [notebook](http://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/Wald_Friedman.ipynb) from the [QuantEcon.Notebooks website](https://quantecon.org/notebooks.html). It introduces the problem that motivated the invention of sequential analysis which was developed by Abraham Wald during WWII. Rather than solve the problem via sequential analysis, we will set it up as a dynamic programming problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using PyPlot\n",
    "using QuantEcon.compute_fixed_point, QuantEcon.DiscreteRV,\n",
    "      QuantEcon.draw, QuantEcon.LinInterp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "On pages 137-139 of **Two Lucky People** by Milton and Rose Friedman, they describe a problem presented to Milton Friedman and Allen Wallis during World War II when they worked at the U.S. government's Statistical Research Group at Columbia University.  \n",
    "\n",
    "> In order to understand the story, it is necessary to have an idea of a simple statistical problem, and of the standard procedure for dealing with it.  The actual problem out of which sequential analysis grew will serve. The Navy has two alternative designs (say A and B) for a projectile.  It wants to determine which is superior. To do so it undertakes a series of paired firings. On each round it assigns the value 1 or 0 to A accordingly as its performance is superior or inferio to that of B and conversely 0 or 1 to B.  The Navy asks the statistician how to conduct the test and how to analyze the results. \n",
    "   \n",
    "> The standard statistical answer was to specify a number of firings (say 1,000) and a pair of percentages (e.g., 53% and 47%) and tell the client that if A receives a 1 in more than 53% of the firings, it can be regarded as superior; if it receives a 1 in fewer than 47%, B can be regarded as superior; if the percentage is between 47% and 53%, neither can be so regarded.\n",
    "   \n",
    "> When Allen Wallis was discussing such a problem with (Navy) Captain Garret L. Schyler, the captain objected that such a test, to quote from Allen's account, may prove wasteful.  If a wise and seasoned ordnance officer like   Schyler were on the premises, he would see after the first few thousand or even few hundred [rounds] that the experiment need not be completed either because he new method is obviously inferior or because it is obviously superior beyond what was hoped for...\"\n",
    "  \n",
    "Friedman and Wallis struggled with the problem but after realizing that they were not able to solve it themselves told Abraham Wald it. That started Wald on the path that led  *Sequential Analysis*. We will instead formulate the problem using dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic programming formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presentation of the problem closely follows Dmitri Berskekas's treatment in **Dynamic Programming and Stochastic Control**.\n",
    "\n",
    "An i.i.d. random variable $z$ can take on values $z \\in [v_1, v_2, \\dots, v_n]$\n",
    "\n",
    "A decision maker wants to  know which of two probability distributions governs  $z$. To formalize this idea,\n",
    "let $x \\in [x_0, x_1]$ be a hidden state that indexes the two distributions:\n",
    "\n",
    "$$P(v_k \\mid x) = \\begin{cases} f_0(v_k) & \\mbox{if } x = x_0, \\\\\n",
    "                                f_1(v_k) & \\mbox{if } x = x_1. \\end{cases}$$                            \n",
    "\n",
    "Before observing any outcomes, a decision maker believes that the probability that $x = x_0$ is $p_{-1} \\in (0,1)$: \n",
    "\n",
    "$$ p_{-1} = \\textrm{Prob}(x=x_0 \\mid \\textrm{ no observations}) $$\n",
    "\n",
    "After observing $k+1$ observations $z_k, z_{k-1}, \\ldots, z_0$ he believes that the probability that the distribution is $f_0$ is\n",
    "\n",
    "$$p_k = \\textrm{Prob} (x=x_0 \\mid z_k, z_{k-1}, \\ldots, z_0) $$\n",
    "\n",
    "We can compute this $p_k$ recursively by applying Bayes' law:\n",
    "\n",
    "$$p_k = \\frac{ p_{k-1} f_0(z_k)}{ p_{k-1} f_0(z_k) + (1-p_{k-1}) f_1(z_k) } $$\n",
    "\n",
    "After observing $z_k, z_{k-1}, \\dots, z_0$, the decision maker believes that $z_{k+1}$ \n",
    "has probability distribution, $f$, given by\n",
    "\n",
    "$$f(z_{k+1}) = p_k f_0(z_{k+1}) + (1-p_k) f_1 (z_{k+1}). $$\n",
    "\n",
    "This is evidently a mixture of distributions $f_0$ and $f_1$, with the weight on $f_0$ being the posterior probability $f_0$ that the distribution is $f_0$. \n",
    "\n",
    "**Remark:** *Because the decision maker believes that $z_{k+1}$ is drawn from a mixture of two i.i.d. distributions, he does *not* believe that the sequence  $[z_{k+1}, z_{k+2}, \\ldots] $ is i.i.d.  Instead, he believes that it is *exchangeable*.  See David Kreps\n",
    "*Notes on the Theory of Choice*, chapter 11, for a discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Mixture Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of two distributions. Here we'll display two beta distributions.  First, we'll show the two distributions, then we'll show mixtures of these same two distributions with various mixing probabilities $p_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two distributions over 50 values for k\n",
    "# We are using a discretized beta distribution\n",
    "function discretize_beta(a, b; p_m1=linspace(0, 1, 50))\n",
    "    f = clamp.(pdf(Beta(a, b), p_m1), 1e-8, Inf)\n",
    "    f = f / sum(f)\n",
    "\n",
    "    return f\n",
    "end\n",
    "    \n",
    "p_m1 = linspace(0, 1, 50)\n",
    "f0 = discretize_beta(1, 1)\n",
    "f1 = discretize_beta(9, 9);\n",
    "f1 = clamp.(pdf(Beta(9, 9), p_m1), 1e-8, Inf)\n",
    "f1 = f1 / sum(f1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[1][:plot](p_m1, f0, label=\"\\$f_0\\$\")\n",
    "ax[1][:plot](p_m1, f1, label=\"\\$f_1\\$\")\n",
    "ax[1][:set_title](\"Original Distributions\")\n",
    "\n",
    "for _p in [0.25, 0.5, 0.75]\n",
    "    ax[2][:plot](p_m1, _p.*f0 + (1.0-_p).*f1, label=\"p = $_p\")\n",
    "end\n",
    "ax[2][:set_title](\"Mixtures Distributions\")\n",
    "\n",
    "function set_features(cax)\n",
    "    cax[:set_xlabel](\"\\$z_k\\$\")\n",
    "    cax[:set_ylabel](\"Prob\\$(z_k)\\$\")\n",
    "    cax[:set_xlim](0.0, 1.0)\n",
    "    cax[:set_ylim](0.0, 0.1)\n",
    "    cax[:legend]()\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "map(set_features, ax)\n",
    "fig[:tight_layout]();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and costs\n",
    "\n",
    "\n",
    "After observing $z_k, z_{k-1}, \\ldots, z_0$, the decision maker chooses among three distinct actions:\n",
    "\n",
    "* He decides that $x = x_0$ and draws no more $z$'s\n",
    "* He decides that $x = x_1$ and draws no more $z$'s\n",
    "* He postpones deciding now and instead chooses to draw a $z_{k+1}$\n",
    "\n",
    "Associated with these three actions, the decision maker suffers three kinds of losses:\n",
    "\n",
    " \n",
    "* A loss $L_0$ if he decides $x = x_0$ when actually $x=x_1$\n",
    "* A loss $L_1$ if he decides $x = x_1$ when actually $x=x_0$\n",
    "* A cost $c$ if he postpones deciding and chooses instead to draw another $z$ \n",
    "\n",
    "For example, suppose that we regard $x=x_0$ as a null hypothesis. Then  \n",
    "\n",
    "* We can think of $L_1$ as the loss associated with a type I error\n",
    "* We can think of $L_0$ as the loss associated with a type II error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "#### How fast can you figure out the distribution?\n",
    "\n",
    "In what follows, we are going to take the role of the decision maker. I will give a set of possible distributions of which one distribution will be drawn randomly then we will simulate draws from them and display them one draw at a time.\n",
    "\n",
    "When you think you know which distribution the draws are coming from, raise your hand.\n",
    "\n",
    "* Would the time at which you raised your hand change if your life depended on it?\n",
    "* Would I charged you a quarter for each draw before you raised your hand?\n",
    "\n",
    "Later, we will allow the model's decision maker to answer these types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function data_generating_process(ab_pairs; N=250)\n",
    "\n",
    "    # Randomly decide between the two sets of parameters\n",
    "    n = length(ab_pairs)\n",
    "    chosendist = rand(collect(1:n))\n",
    "    a, b = ab_pairs[chosendist]\n",
    "\n",
    "    rand_draws = rand(Beta(a, b), N)\n",
    "\n",
    "    return chosendist, rand_draws\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy case\n",
    "\n",
    "We will begin with two easily distinguishable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_pairs = [(6.0, 6.0), (1.0, 1.0)]\n",
    "N = 50\n",
    "\n",
    "p_m1 = linspace(0, 1, 50)\n",
    "f0 = discretize_beta(ab_pairs[1][1], ab_pairs[1][2])\n",
    "f1 = discretize_beta(ab_pairs[2][1], ab_pairs[2][2])\n",
    "\n",
    "fig, ax = subplots()\n",
    "ax[:plot](p_m1, f0, label=\"\\$f_0\\$\")\n",
    "ax[:plot](p_m1, f1, label=\"\\$f_1\\$\")\n",
    "ax[:legend]()\n",
    "fig[:suptitle](\"Easily Distinguishable Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdgp, data = data_generating_process(ab_pairs; N=N)\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "for n=1:N\n",
    "    cla()\n",
    "\n",
    "    ax[:hist](data[1:n], animated=true, normed=false, bins=15)\n",
    "    ax[:set_xlim](0.0, 1.0)\n",
    "    ax[:set_ylim](0.0, 25.0)\n",
    "\n",
    "    sleep(0.05)\n",
    "    IJulia.clear_output(true)\n",
    "    display(fig)\n",
    "end\n",
    "clf()\n",
    "\n",
    "println(\"The correct answer was $(tdgp-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenging Case\n",
    "\n",
    "Now, we complicate matters by choosing more similar distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_pairs = [(8.5, 9.0), (9.0, 8.5)]\n",
    "N = 125\n",
    "\n",
    "p_m1 = linspace(0, 1, 50)\n",
    "f0 = discretize_beta(ab_pairs[1][1], ab_pairs[1][2])\n",
    "f1 = discretize_beta(ab_pairs[2][1], ab_pairs[2][2])\n",
    "\n",
    "fig, ax = subplots()\n",
    "ax[:plot](p_m1, f0, label=\"\\$f_0\\$\")\n",
    "ax[:plot](p_m1, f1, label=\"\\$f_1\\$\")\n",
    "ax[:legend]()\n",
    "fig[:suptitle](\"Difficult to Distinguish Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdgp, data = data_generating_process(ab_pairs; N=N)\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "for n=1:N\n",
    "    cla()\n",
    "\n",
    "    ax[:hist](data[1:n], animated=true, normed=false, bins=15)\n",
    "    ax[:set_xlim](0.0, 1.0)\n",
    "    ax[:set_ylim](0.0, 25.0)\n",
    "\n",
    "    IJulia.clear_output(true)\n",
    "    display(fig)\n",
    "end\n",
    "clf()\n",
    "\n",
    "println(\"The correct answer was $(tdgp-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning to the math\n",
    "\n",
    "#### A Bellman equation\n",
    "\n",
    "Let $J(p_k)$ be the total loss for a decision maker who with posterior probability $p_k$ who chooses optimally.\n",
    "\n",
    "The loss function $J(p_k)$ satisfies the Bellman equation\n",
    "\n",
    "$$ J(p_k) = \\min \\left\\{ (1-p_k) L_0, p_k L_1, c + E_{z_{k+1}} \\left[ J (p_{k+1}) \\right] \\right\\} $$\n",
    "\n",
    "where $E_{z_{k+1}}$ denotes a mathematical expectation (using today's beliefs!) over the distribution of $z_{k+1}$ and  the minimization is over the three actions, accept $x_0$, accept $x_1$, and postponing by drawing $z_{k+1}$.\n",
    "\n",
    "Let \n",
    "\n",
    "$$A(p_k) = E_{z_{k+1}} \\left[ J \\left( \\frac{ p_k f_0(z_{k+1})}{ p_k f_0(z_{k+1}) + (1-p_k) f_1 (z_{k+1}) } \\right) \\right] $$\n",
    "\n",
    "Then we can write out Bellman equation as\n",
    "\n",
    "$$ J(p_k) = \\min \\left[ (1-p_k) L_0, p_k L_1, c + A(p_k) \\right] $$\n",
    "\n",
    "where $p_k \\in [0,1]$.  \n",
    "\n",
    "One can show that the optimal decision rule is characterized by two probabilities $0 < \\beta < \\alpha < 1$ such that\n",
    "\n",
    "$$(1- p_k) L_0 < \\min \\{ p_k L_1, c + A_k(p_k) \\}  \\textrm { if } p_k \\geq \\alpha_k  $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ p_k L_1 < \\min \\{ (1-p_k) L_0,  c + A_k(p_k) \\} \\textrm { if } p_k \\leq \\beta_k $$\n",
    "\n",
    "The optimal decision rule is then\n",
    "\n",
    "$$ \\textrm { accept } x=x_0 \\textrm{ if } p_k \\geq \\alpha_k \\\\\n",
    "   \\textrm { accept } x=x_1 \\textrm{ if } p_k \\leq \\beta_k \\\\\n",
    "   \\textrm { draw another }  z \\textrm{ if }  \\beta_k \\leq p_k \\leq \\alpha_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Bellman equation\n",
    "\n",
    "To approximate the solution of the Bellman equation above, we can deploy a method known as value function iteration. Value function iteration consists of starting with an initial guess of the value function and then evaluating the Bellman equation by using the guess to evaluate the expectation. There is theory that says this should converge to the \"true\" value function.\n",
    "\n",
    "Numerically, this will consist of guessing the value of a function over a grid of points and then updating the values on that grid. Because we are iterating on a grid, the current probability, $p_k$, is restricted to a set number of points. However, in order to evaluate the expectation of the Bellman equation for tomorrow, $A(p_{k})$, we must be able to evaluate at various $p_{k+1}$ which may or may not correspond with points on our grid. The way that we resolve this issue is by using *linear interpolation*. This means to evaluate $J(p)$ where $p$ is not a grid point, we must use two points: first, we use the largest of all the grid points smaller than $p$, and call it $p_i$, and, second, we use the grid point immediately after $p$, named $p_{i+1}$, to approximate the function value in the following manner:\n",
    "\n",
    "$$ J(p) = J(p_i) + (p - p_i) \\frac{J(p_{i+1}) - J(p_i)}{p_{i+1} - p_{i}}$$\n",
    "\n",
    "In one dimension, you can think of this as simply drawing a line between each pair of points on the grid.\n",
    "\n",
    "For more information on both linear interpolation and value function iteration methods, see the Quant-Econ [lecture](http://quant-econ.net/py/ifp.html) on the income fluctuation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia Implementation\n",
    "\n",
    "### Define relevant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This type is used to store the solution to the problem presented \n",
    "in the \"Wald Friedman\" notebook presented on the QuantEcon website.\n",
    "\n",
    "Solution\n",
    "----------\n",
    "J : vector(Float64)\n",
    "    Discretized value function that solves the Bellman equation\n",
    "β : scalar(Real)\n",
    "    Lower cutoff for continuation decision\n",
    "α : scalar(Real)\n",
    "    Upper cutoff for continuation decision\n",
    "\"\"\"\n",
    "mutable struct WFSolution\n",
    "    J::Vector{Float64}\n",
    "    β::Float64\n",
    "    α::Float64\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "This type is used to solve the problem presented in the \"Wald Friedman\"\n",
    "notebook presented on the QuantEcon website.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "c : scalar(Real)\n",
    "    Cost of postponing decision\n",
    "L0 : scalar(Real)\n",
    "    Cost of choosing model 0 when the truth is model 1\n",
    "L1 : scalar(Real)\n",
    "    Cost of choosing model 1 when the truth is model 0\n",
    "f0 : vector(Float64)\n",
    "    A finite state probability distribution\n",
    "f1 : vector(Float64)\n",
    "    A finite state probability distribution\n",
    "m : scalar(Int64)\n",
    "    Number of points to use in function approximation\n",
    "\"\"\"\n",
    "struct WaldFriedman\n",
    "    c::Float64\n",
    "    L0::Float64\n",
    "    L1::Float64\n",
    "    f0::Vector{Float64}\n",
    "    f1::Vector{Float64}\n",
    "    m::Int64\n",
    "    pgrid::LinSpace{Float64}\n",
    "    sol::WFSolution\n",
    "end\n",
    "\n",
    "function WaldFriedman(c, L0, L1, f0, f1)\n",
    "    # Make sure distributions have same number\n",
    "    # of elements\n",
    "    m = length(f0)\n",
    "    @assert m == length(f1)\n",
    "\n",
    "    pgrid = linspace(0.0, 1.0, m)\n",
    "\n",
    "    # Renormalize distributions so nothing is \"too\" small\n",
    "    f0 = clamp.(f0, 1e-8, 1-1e-8)\n",
    "    f1 = clamp.(f1, 1e-8, 1-1e-8)\n",
    "    f0 = f0 ./ sum(f0)\n",
    "    f1 = f1 ./ sum(f1)\n",
    "\n",
    "    # Guess the value function is 0 everywhere\n",
    "    J = zeros(m)\n",
    "    beta = 0.\n",
    "    alpha = 1.\n",
    "    \n",
    "    WaldFriedman(c, L0, L1, f0, f1, m, pgrid, WFSolution(J, beta, alpha))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes a value for the probability with which\n",
    "the correct model is model 0 and returns the mixed\n",
    "distribution that corresponds with that belief.\n",
    "\"\"\"\n",
    "function current_distribution(wf::WaldFriedman, p::Float64)\n",
    "\n",
    "    return p*wf.f0 + (1-p)*wf.f1\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "This function takes a value for p, and a realization of the\n",
    "random variable and calculates the value for p tomorrow.\n",
    "\"\"\"\n",
    "function bayes_update(wf::WaldFriedman, p::Float64, k::Int)\n",
    "    f0_k = wf.f0[k]\n",
    "    f1_k = wf.f1[k]\n",
    "\n",
    "    p_tp1 = p*f0_k / (p*f0_k + (1-p)*f1_k)\n",
    "\n",
    "    return clamp.(p_tp1, 0, 1)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "This is similar to `bayes_update_k` except it returns a\n",
    "new value for p for each realization of the random variable\n",
    "\"\"\"\n",
    "function bayes_update(wf::WaldFriedman, p::Float64)\n",
    "\n",
    "    return bayes_update.(wf, p, 1:wf.m)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "For a given probability specify the cost of accepting model 0\n",
    "\"\"\"\n",
    "function payoff_choose_f0(wf::WaldFriedman, p)\n",
    "\n",
    "    return (1-p)*wf.L0\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "For a given probability specify the cost of accepting model 1\n",
    "\"\"\"\n",
    "function payoff_choose_f1(wf::WaldFriedman, p)\n",
    "\n",
    "    return p*wf.L1\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "This function evaluates the expectation of the value function\n",
    "at period t+1. It does so by taking the current probability\n",
    "distribution over outcomes:\n",
    "\n",
    "    p(z_{k+1}) = p_k f_0(z_{k+1}) + (1-p_k) f_1(z_{k+1})\n",
    "\n",
    "and evaluating the value function at the possible states\n",
    "tomorrow J(p_{t+1}) where\n",
    "\n",
    "    p_{t+1} = p f0 / ( p f0 + (1-p) f1)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "p : scalar\n",
    "    The current believed probability that model 0 is the true\n",
    "    model.\n",
    "J : function (interpolant)\n",
    "    The current value function for a decision to continue\n",
    "\n",
    "Returns\n",
    "-------\n",
    "EJ : scalar\n",
    "    The expected value of the value function tomorrow\n",
    "\"\"\"\n",
    "function EJ(wf::WaldFriedman, p, J)\n",
    "    # Get the current believed distribution (to use in taking\n",
    "    # the expectation)\n",
    "    curr_dist = current_distribution(wf, p)\n",
    "\n",
    "    # Evaluate the new belief tomorrow for each possible realization\n",
    "    tp1_dist = bayes_update(wf, p)\n",
    "   \n",
    "    # Dot product of my current beliefs with value function at\n",
    "    # each p tomorrow gives me expected value\n",
    "    EJ = dot(curr_dist, J.(tp1_dist))\n",
    "\n",
    "    return EJ\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "For a given probability distribution and value function give\n",
    "cost of continuing the search for correct model\n",
    "\"\"\"\n",
    "function payoff_continue(wf::WaldFriedman, p, J)\n",
    "    return wf.c + EJ(wf, p, J)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "This function takes a value function and returns the corresponding\n",
    "cutoffs of where you transition between continue and choosing a\n",
    "specific model\n",
    "\"\"\"\n",
    "function find_cutoff_rule(wf::WaldFriedman, J)\n",
    "    m, pgrid = wf.m, wf.pgrid\n",
    "\n",
    "    # Evaluate cost at all points on grid for choosing a model\n",
    "    p_c_0 = payoff_choose_f0(wf, pgrid)\n",
    "    p_c_1 = payoff_choose_f1(wf, pgrid)\n",
    "\n",
    "    # The cutoff points can be found by differencing these costs with\n",
    "    # the Bellman equation (J is always less than or equal to p_c_i)\n",
    "    beta = pgrid[searchsortedlast(p_c_1 - J, 1e-10)]\n",
    "    alpha = pgrid[searchsortedlast(J - p_c_0, -1e-10)]\n",
    "\n",
    "    return beta, alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluates the value function for a given continuation value\n",
    "function; that is, evaluates\n",
    "\n",
    "    J(p) = min( (1-p)L0, pL1, c + E[J(p')])\n",
    "\n",
    "Uses linear interpolation between points\n",
    "\"\"\"\n",
    "function bellman_operator(wf::WaldFriedman, J)\n",
    "    c, L0, L1, f0, f1 = wf.c, wf.L0, wf.L1, wf.f0, wf.f1\n",
    "    m, pgrid = wf.m, wf.pgrid\n",
    "\n",
    "    J_out = zeros(m)\n",
    "    J_interp = LinInterp(pgrid, J)\n",
    "    \n",
    "    for (p_ind, p) in enumerate(pgrid)\n",
    "        # Payoff of choosing model 0\n",
    "        p_c_0 = payoff_choose_f0(wf, p)\n",
    "        p_c_1 = payoff_choose_f1(wf, p)\n",
    "        p_con = payoff_continue(wf, p, J_interp)\n",
    "\n",
    "        J_out[p_ind] = min(p_c_0, p_c_1, p_con)\n",
    "    end\n",
    "\n",
    "    return J_out\n",
    "end\n",
    "\n",
    "function solve_model(wf; tol=1e-7)\n",
    "    bell_op(x) = bellman_operator(wf, x)\n",
    "    J =  compute_fixed_point(bell_op, zeros(wf.m), err_tol=tol, print_skip=5)\n",
    "\n",
    "    wf.sol.J = J\n",
    "    wf.sol.β, wf.sol.α = find_cutoff_rule(wf, J)\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write a simulation function\n",
    "\n",
    "There is a template below (the answer is also below)... It should take a model, a true distribution, and a prior that $f_0$ is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes an initial condition and simulates until it\n",
    "stops (when a decision is made).\n",
    "\"\"\"\n",
    "function simulate(wf::WaldFriedman, f; p0=0.5)\n",
    "    # Check whether vf is computed\n",
    "    if sum(abs, wf.sol.J) < 1e-8\n",
    "        solve_model(wf)\n",
    "    end\n",
    "        \n",
    "    # Unpack useful info\n",
    "    beta, alpha = wf.sol.β, wf.sol.α\n",
    "    drv = DiscreteRV(f)\n",
    "\n",
    "    # Initialize a couple useful variables\n",
    "    decision_made = false\n",
    "    decision = 0\n",
    "    p = p0\n",
    "    t = 0\n",
    "\n",
    "    # While some condition is true\n",
    "    while (???)\n",
    "        # Draw an a new realization\n",
    "        k = rand(drv)[1]\n",
    "        t = t+1\n",
    "\n",
    "        # Update according to Bayes law\n",
    "        p = bayes_update(wf, p, k)\n",
    "        # what happens if p is below beta?\n",
    "        if (???)\n",
    "\n",
    "        # What happens if p is above alpha\n",
    "        elseif (???)\n",
    "\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    return decision, p, t\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button data-toggle=\"collapse\" data-target=\"#sol2\" class='btn btn-primary'>One Possible Solution</button>\n",
    "<div id=\"sol2\" class=\"collapse\">\n",
    "\n",
    "```julia\n",
    "\"\"\"\n",
    "This function takes an initial condition and simulates until it\n",
    "stops (when a decision is made).\n",
    "\"\"\"\n",
    "function simulate(wf::WaldFriedman, f; p0=0.5)\n",
    "    # Check whether vf is computed\n",
    "    if sum(abs, wf.sol.J) < 1e-8\n",
    "        solve_model(wf)\n",
    "    end\n",
    "        \n",
    "    # Unpack useful info\n",
    "    beta, alpha = wf.sol.β, wf.sol.α\n",
    "    drv = DiscreteRV(f)\n",
    "\n",
    "    # Initialize a couple useful variables\n",
    "    decision_made = false\n",
    "    decision = 0\n",
    "    p = p0\n",
    "    t = 0\n",
    "\n",
    "    while !decision_made\n",
    "        # Draw an a new realization\n",
    "        k = rand(drv)[1]\n",
    "        t = t+1\n",
    "\n",
    "        # Update according to Bayes law\n",
    "        p = bayes_update(wf, p, k)\n",
    "        if p < beta\n",
    "            decision_made = true\n",
    "            decision = 1\n",
    "        elseif p > alpha\n",
    "            decision_made = true\n",
    "            decision = 0\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    return decision, p, t\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses the distribution f0 as the true data generating\n",
    "process\n",
    "\"\"\"\n",
    "function simulate_tdgp_f0(wf::WaldFriedman; p0=0.5)\n",
    "    decision, p, t = simulate(wf, wf.f0; p0=p0)\n",
    "\n",
    "    if decision == 0\n",
    "        correct = true\n",
    "    else\n",
    "        correct = false\n",
    "    end\n",
    "        \n",
    "    return correct, p, t\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Uses the distribution f1 as the true data generating\n",
    "process\n",
    "\"\"\"\n",
    "function simulate_tdgp_f1(wf::WaldFriedman; p0=0.5)\n",
    "    decision, p, t = simulate(wf, wf.f1; p0=p0)\n",
    "\n",
    "    if decision == 1\n",
    "        correct = true\n",
    "    else\n",
    "        correct = false\n",
    "    end\n",
    "        \n",
    "    return correct, p, t\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Simulates repeatedly to get distributions of time needed to make a\n",
    "decision and how often they are correct.\n",
    "\"\"\"\n",
    "function stopping_dist(wf::WaldFriedman; ndraws=250, tdgp=\"f0\")\n",
    "    if tdgp==\"f0\"\n",
    "        simfunc = simulate_tdgp_f0\n",
    "    else\n",
    "        simfunc = simulate_tdgp_f1\n",
    "    end\n",
    "        \n",
    "    # Allocate space\n",
    "    tdist = Array{Int64}(ndraws)\n",
    "    cdist = Array{Bool}(ndraws)\n",
    "\n",
    "    for i in 1:ndraws\n",
    "        correct, p, t = simfunc(wf)\n",
    "        tdist[i] = t\n",
    "        cdist[i] = correct\n",
    "    end\n",
    "        \n",
    "    return cdist, tdist\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our types to solve the Bellman equation (*) and check whether it gives the same answer attained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Example\n",
    "\n",
    "Now let's specify the two probability distibutions (the ones that we plotted earlier)\n",
    "\n",
    "* for $f_0$ we'll assume a beta distribution with parameters $a=1, b=1$\n",
    "\n",
    "* for $f_1$ we'll assume a beta distribution with parameters $a=9, b=9$\n",
    "\n",
    "The density of a  beta probability distribution with parameters $a$ and $b$ is\n",
    "\n",
    "$$ f(z; a, b) = \\frac{\\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\\Gamma(a) \\Gamma(b)}$$\n",
    "\n",
    "where $\\Gamma$ is the gamma function \n",
    "\n",
    "$$\\Gamma(t) = \\int_{0}^{\\infty} x^{t-1} e^{-x} dx$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose parameters\n",
    "c = 1.25\n",
    "L0 = 27.0\n",
    "L1 = 27.0\n",
    "\n",
    "# Choose n points and distributions\n",
    "m = 251\n",
    "f0 = pdf(Beta(2.5, 3), linspace(0, 1, m))\n",
    "f0 = f0 / sum(f0)\n",
    "f1 = pdf(Beta(3, 2.5), linspace(0, 1, m))\n",
    "f1 = f1 / sum(f1)  # Make sure sums to 1\n",
    "\n",
    "# Create an instance of our WaldFriedman class\n",
    "wf = WaldFriedman(c, L0, L1, f0, f1);\n",
    "\n",
    "solve_model(wf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve and simulate the solution\n",
    "cdist, tdist = stopping_dist(wf; ndraws=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function equals $ p L_1$ for $p \\leq \\alpha$, and $(1-p )L_0$ for $ p \\geq \\beta$.\n",
    "Thus, the slopes of the two linear pieces of the value function are determined by $L_1$ and \n",
    "$- L_0$.  \n",
    "\n",
    "The value function is smooth in the interior region in which the probability assigned to distribution  $f_0$ is in the indecisive region $p \\in (\\alpha, \\beta)$.\n",
    "\n",
    "The decision maker continues to sample until the probability that he attaches to model $f_0$ falls below $\\alpha$ or above $\\beta$.\n",
    "\n",
    "The value function is smooth in the interior region in which the probability assigned to distribution  $f_0$ is in the indecisive region $p \\in (\\alpha, \\beta)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots()\n",
    "\n",
    "ax[:plot](wf.pgrid, wf.sol.J)\n",
    "\n",
    "maxJ = maximum(wf.sol.J)\n",
    "ax[:vlines](wf.sol.β, 0.0, wf.sol.β*wf.L1, \"k\", \"--\")\n",
    "ax[:annotate](L\"$\\beta$\", (wf.sol.β-0.05, maxJ - 1.0))\n",
    "ax[:vlines](wf.sol.α, 0.0, (1.0 - wf.sol.α)*wf.L0, \"k\", \"--\")\n",
    "ax[:annotate](L\"$\\alpha$\", (wf.sol.α+0.05, maxJ - 1.0))\n",
    "\n",
    "fig[:suptitle](\"Value Function\")\n",
    "ax[:set_xlabel](\"\\$p\\$\")\n",
    "ax[:set_ylabel](\"\\$J(p)\\$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to have some fun, you can change the cost parameters $L_0, L_1, c$, the parameters of two beta distributions $f_0$ and $f_1$, and the number of points and linear functions $m$ to use in our piece-wise continuous approximation to the value function. You can see the effects on the smoothness of the value function in the  middle range as you increase the numbers of functions in the piecewise linear approximation.  \n",
    "\n",
    "The function `stopping_dist` draws a number of simulations from $f_0$, computes a distribution of waiting times to making a decision, and displays a histogram of correct and incorrect decisions. (Here the correct decision occurs when $p_k$ eventually exceeds $\\beta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, allax = subplots(2, 2)\n",
    "\n",
    "ax1, ax2 = allax[1, 1], allax[1, 2]\n",
    "ax3, ax4 = allax[2, 1], allax[2, 2]\n",
    "\n",
    "ax1[:plot](wf.pgrid, wf.f0, label=L\"$f_0$\")\n",
    "ax1[:plot](wf.pgrid, wf.f1, label=L\"$f_1$\")\n",
    "ax1[:set_title](\"Distributions\")\n",
    "ax1[:set_xlabel](L\"$v_k$\")\n",
    "ax1[:set_xlabel](L\"$f(v_k)$\")\n",
    "\n",
    "ax2[:plot](wf.pgrid, wf.sol.J)\n",
    "ax2[:set_title](\"Value Function\")\n",
    "ax2[:set_xlabel](L\"$p_k$\")\n",
    "ax2[:set_ylabel](L\"$J(p_k)$\")\n",
    "\n",
    "count_stop_time = Array{Int64}(maximum(tdist))\n",
    "for i in 1:maximum(tdist)\n",
    "    count_stop_time[i] = sum(tdist .== i)\n",
    "end\n",
    "ax3[:bar](collect(1:maximum(tdist)), count_stop_time)\n",
    "ax3[:set_title](\"Stopping Times\")\n",
    "ax3[:set_xlabel](L\"$T$\")\n",
    "ax3[:set_ylabel](\"Frequency\")\n",
    "\n",
    "ncorrect = sum(cdist)\n",
    "nincorrect = length(cdist) - ncorrect\n",
    "percorr = round(100*mean(cdist), 2)\n",
    "ax4[:bar]([0, 1], [nincorrect, ncorrect])\n",
    "ax4[:annotate](\"$percorr\\% Correct\", [-0.45, nincorrect+200])\n",
    "ax4[:set_title](\"Decisions\")\n",
    "ax4[:set_xticks]([0.0, 1.0])\n",
    "ax4[:set_xticklabels]([\"Incorrect\", \"Correct\"])\n",
    "\n",
    "fig[:tight_layout]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.1-pre",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
